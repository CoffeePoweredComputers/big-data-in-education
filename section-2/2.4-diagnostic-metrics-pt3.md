# Diagnostic Metrics - Metrics for Regressors

##### Linear Correlation - Pearson's Correlation
* r(A, B) = 
* Assumes a linear relationship between A and B.
* Correlations
  * 1.0 -> Perfect
  * 0.0 -> None
  * -1.0 -> Perfectly Negatively Correlated.
* Interpretation:
  * The same function can be modeled by completely different data.

* r^2
  * The correlation squared
  * A measure of what percentage of variance in dependent measure is explained by a model.

##### Spearman's Correlation 
* The 'Rank Correlation':
  * Turn each variable into ranks:
    * 1 -> highest
    * 2 -> 2nd highest
    * and so forth...
  * Compute Pearson's Correlation
* Useful for:
  * It's more robust to outliers.
  * Determines how monotonic a relationship is - how much one going up associates with another going down - but not how linear it is.
* Interpreted exactly the same way as Pearson's correlation.

##### Root Mean Square Error (RMSE) or Mean Absolute Deviation (MAD):
* Mean Absolute Deviation -> MAD tells you the average amount to which the predictions deviate from the actual value.
* Root Mean Square -> RMSE can be interpreted the same way (mostly) but penalizes large deviation more than small deviation.
* Interpretation:
  * Low RMSE/MAD, High Correlation -> Good model
  * High RMSE/MAD, Low Correlation -> Good model
  * High RMSE/MAD, High Correlation -> Good model but is systematically biased.
  * Low RMSE/MAD, Low Correlation -> Good model but the model doesn't capture relative change. This is particuarly common if there's not much variance.

##### Bayesian Information Criterion (BiC): 
* Makes trade off between goodness of fit and flexibility of fit (# of parameters).
* Formula for linear regression -> BiC' = nlog(1-r^2) + plog(n)
  * n -> number of students
  * p -> number of variables
* values over 0 -> Worse than expected given number of variables.
* Values under 0 -> Better than expected given number of variables.
* Said to be statistically equivalent to k-fold cross-validation for optimal k.
* Easier to compute than cross-validation, but different formulas must be used for different modeling frameworks.

##### An information Criterion (AIC):
* Makes slight different trade-off between goodness of fit and flexibility of fit.
* It's said to be statistically equivalent to leave out one cross-validation.
