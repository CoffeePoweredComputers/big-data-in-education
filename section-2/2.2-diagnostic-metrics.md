# Diagnostic Metrics - Metrics for Classifiers

##### Accuracy:
* One of the oldest/simplest measures.
* Called *agreement* when measuring inter-rater reliability.
* Accuracy is not a good metric as it does poorly when there is non-even assignment to categories, which is almost always the case.

##### Kappa:
* agreement - expected agreement) / (1 - expected agreement)
* Kappa is 0 -> agreement is at chance
* Kappa is 1 -> Agreement is perfect
* Kappa is -1 -> Agreement is worse than chance
* In data mining 0.3-0.5 is generally good enough to be considered publishable
* Comparing kappa values between two data sets, in a principled fashion, is highly difficult.
* Informally, you can compare two data sets if the proportions of each category are similar.

