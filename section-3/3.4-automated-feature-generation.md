# Automated Feature Generation

* The creation of new data features in an automated fashion from existing data features.

##### Multiplicative Interactions:
* Steps:
  * You have variables A and B
  * New variable C= A * B
  * Do this for all possible variables
* Variant using A/B with the added decision of what to do when B=0.

##### Automated Threshold Selection:
* Tran a numerical variable into a binary.
* Try to find the cut-off point that maximizes your depended variable.

##### Why do this?:
* Finalizing numerical variables by finding thresholds and running linear regression even though it is the same or similar to J48.

##### Auto-Encoders:
* Uses neural network to find structure in variables in an unsupervised fashion.

##### Automated Features Selection:
* The process of selecting features prior to running an algorithm.
*  Doing automated features election on your whole data set prior to building models raises the chances of over-fitting.
* Approaches:
  * Correlation Filtering:
    * Throw out variables that are too closely correlated to each other. 
    * An arbitrary decision, and sometimes the better variables get filtered.
  * Fast Correlation-Based Filtering:
    * Find the correlation between each pair of matures or other measure of relatedness.
    * Sort the features by their correlation to the predicted variable.
    * Throw out all the other features that are too highly correlated to that best feature.
    * Take all other features, and repeat the process.
    * This gives you a set of variables that aren't too highly correlated to each other, but are well-correlated to the predicted variable, or at least relatively welly-well correlated tithe predicted variable.
  * Removing features that could have second-order effects:
    * Run your algorithm with each feature alone
    * Throw out all variables that are equal to or worse than chance in a single-feature model.
    * Reduces the scope for over-fitting but also for finding genuine second-order effects.
  * Forward Selection:
    * Another thing you can do is introduce an outer-loop forward selection procedure outside your algorithm.
    * Steps:
      * On other words, try running your algorithm on every variable individually.
      * Take the best model and keep that variable.
      * Now try running your algorithm using that variable and, in addition, each other variable.
      * take the best model, and keep both vars.
      * Repete until there are no variables added that make the model better under cross validation.
    * This find the best set of variables rather than finding the goodness of the best model selected out of the whole data set.
    * Improves performance on the current data set but can lead to over fitting, and over estimation of model goodness.
    * But may lead to better performance on a held-out test-set than a model built using all variables.

##### Why do this?
* Feature selection methods are a way of making your overall process more conservative.
* Valuable when you want to under-fit.

